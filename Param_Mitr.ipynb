{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model With GPU (Fine tuning the model using LoRA)","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, load_from_disk\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, GenerationConfig\nimport torch\nfrom peft import LoraConfig, TaskType, get_peft_model, PeftModel, PeftConfig\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T06:54:20.723761Z","iopub.execute_input":"2025-04-09T06:54:20.724114Z","iopub.status.idle":"2025-04-09T06:54:47.870790Z","shell.execute_reply.started":"2025-04-09T06:54:20.724085Z","shell.execute_reply":"2025-04-09T06:54:47.869896Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Import the data from Hugging face","metadata":{"execution":{"iopub.status.busy":"2025-04-09T06:53:56.303114Z","iopub.execute_input":"2025-04-09T06:53:56.303419Z","iopub.status.idle":"2025-04-09T06:53:56.307613Z","shell.execute_reply.started":"2025-04-09T06:53:56.303386Z","shell.execute_reply":"2025-04-09T06:53:56.306607Z"}}},{"cell_type":"code","source":"# dataset = load_dataset('samhog/psychology-10k', split = ['train'])[0]\n# dataset.save_to_disk('./dataset')\n\ndataset = load_from_disk('./dataset')\ndataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"let's walk trough the data","metadata":{}},{"cell_type":"code","source":"def print_conv(me, model):\n    for i, j in zip(me, model):\n        print(\n            f\"\"\"\nMe : {i}\n\nParam mitr : {j}\\n\\n\n            \"\"\"\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_conv(dataset['input'][:3], dataset['output'][:3])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Dataset = dataset.train_test_split(test_size = 0.2)\nDataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import our model with its tokenizer","metadata":{}},{"cell_type":"code","source":"# if you want to use QLoRA use this but remember in \n# normal inference you have to use GPU\n\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=True,\n#     bnb_4bit_quant_type=\"nf4\",\n#     bnb_4bit_compute_dtype=torch.bfloat16,\n#     bnb_4bit_use_double_quant=True,\n#     bnb_4bit_quant_storage=torch.bfloat16,\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importing model from the Hugging Face hub\n\nmodel_name = 'google/flan-t5-small'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype = torch.bfloat16,\n    # quantization_config = bnb_config\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Saving the model\n\n# model.save_pretrained('/kaggle/working/Model')\n# tokenizer.save_pretrained('/kaggle/working/Model')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's try some stuff with tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer(\"hey\", return_token_type_ids = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda')\ntokenizer.decode(\n    model.generate(\n        tokenizer(\n            dataset['input'][0], return_token_type_ids = False, return_tensors = 'pt', padding=True, truncation=True\n        )['input_ids'].to(device)\n    )[0], skip_special_tokens= True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference(input_data, param_mitr = model):\n    intruct = dataset['instruction'][0]\n    task = \"Answer :\"\n    inp = [intruct + \"\\n\" + sent + task for sent in input_data]\n    output = param_mitr.generate(\n        tokenizer(\n            inp, return_token_type_ids = False, return_tensors = 'pt', padding=True, truncation=True\n        )['input_ids'].to(device),\n        generation_config = GenerationConfig(max_new_token = 200)\n    )\n    decoded = [tokenizer.decode(out, skip_special_tokens= True) for out in output]\n    print_conv(input_data, decoded)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inference(dataset['input'][:4])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model is showing good result on ICL (In Context Learning) with zero shot inference.","metadata":{}},{"cell_type":"code","source":"def tokenize_function(example):\n    intruct = dataset['instruction'][0]\n    task = \"Answer :\"\n    inp = [intruct + \"\\n\" + question + '\\n' + task for question in example['input']]\n    example['input_ids'] = tokenizer(inp, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    example['labels'] = tokenizer(example['output'], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n\n    return example\n\n\ntokenized_datasets = Dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['input', 'output', 'instruction'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine tune our model (Using LoRA)","metadata":{}},{"cell_type":"code","source":"config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    r = 32,\n    target_modules = \"all-linear\",\n    lora_alpha=32,\n    lora_dropout=0.05\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Model = get_peft_model(model, config)\nModel.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TrainArgs = TrainingArguments(\n    output_dir = './Model',\n    learning_rate = 1e-3,\n    num_train_epochs = 2,\n    per_device_train_batch_size = 4,\n    per_device_eval_batch_size = 4,\n    weight_decay = 0.01,\n    eval_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    logging_steps=1,\n    load_best_model_at_end = True,\n    report_to = ['tensorboard']\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model = Model,\n    args = TrainArgs,\n    train_dataset = tokenized_datasets['train'],\n    eval_dataset = tokenized_datasets['test']\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\n# To clear the GPU\ndef clean():\n    for i in range(15):\n      gc.collect()\n      torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean()\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def peft_infrence_with_GPU(input_data, model = Model):\n    intruct = dataset['instruction'][0]\n    task = \"Answer :\"\n    inp = [intruct + \"\\n\" + sent +'\\n'+ task for sent in input_data]\n    output = model.generate(\n        **tokenizer(\n            inp, return_token_type_ids = False, return_tensors = 'pt', padding=True, truncation=True\n        ).to('cuda'),\n        generation_config = GenerationConfig(max_new_tokens=100, num_beams=1)\n    )\n    decoded = [tokenizer.decode(out, skip_special_tokens= True) for out in output]\n    print_conv(input_data, decoded)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"peft_infrence_withGPU(Dataset['test']['input'][:4])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.model.save_pretrained('./peft_for_param_mitr')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model without GPU (using saved fine-tuned model)","metadata":{}},{"cell_type":"markdown","source":"## Load model from local storage","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('/kaggle/working/Model', device_map=\"auto\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained('/kaggle/working/Model',\n                                              device_map=\"auto\",\n                                              torch_dtype = torch.bfloat16\n                                             )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load PEFT model ","metadata":{}},{"cell_type":"code","source":"peft_model_path = './peft_for_param_mitr'\nModel = PeftModel.from_pretrained(\n            model,\n            peft_model_path,\n            is_trainable = False\n            )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def peft_infrence(input_data, model = Model):\n    intruct = dataset['instruction'][0]\n    task = \"Answer :\"\n    inp = [intruct + \"\\n\" + sent +'\\n'+ task for sent in input_data]\n    output = model.generate(\n        **tokenizer(\n            inp, return_token_type_ids = False, return_tensors = 'pt', padding=True, truncation=True\n        ),\n        generation_config = GenerationConfig(max_new_tokens=200, num_beams=1)\n    )\n    decoded = [tokenizer.decode(out, skip_special_tokens= True) for out in output]\n    print_conv(input_data, decoded)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"peft_infrence(Dataset['test']['input'][:4])","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}